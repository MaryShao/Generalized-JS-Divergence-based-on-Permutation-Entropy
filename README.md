# Generalized-JS-Divergence-based-on-Permutation-Entropy
Applied information entropy and fractional decomposition to generalize KL and JS divergence. 

The generalized divergence is applied to PCA instead of traditional similarity measurement for dimensional reduction.

We proposed a new JS (Jenson Shannon) divergence method based on permutation entropy;
Generated different 2D ARFIMA time series for a given coupling strength; using them to test JSD, 
fractional JSD and generalized JSD methods based on permutation entropy generalization, 
and generalized fractional JSD method to identify the similarity of two artificial time series.


Established a suitable moving window length, traversing the entire time series, and applied it to the new method, 
using Python to plot the similarity curve between the simulated data and the real stock data 
(seven different regional stock KLCI records); plotted genetic tree, and heat effect map by R 
to visualize the comparison results, and used financial market basic knowledge to verify the accuracy of the results.
